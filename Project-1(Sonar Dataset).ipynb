{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>attribute_61</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0         0.0200       0.0371       0.0428       0.0207       0.0954   \n",
       "1         0.0453       0.0523       0.0843       0.0689       0.1183   \n",
       "2         0.0262       0.0582       0.1099       0.1083       0.0974   \n",
       "3         0.0100       0.0171       0.0623       0.0205       0.0205   \n",
       "4         0.0762       0.0666       0.0481       0.0394       0.0590   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "203       0.0187       0.0346       0.0168       0.0177       0.0393   \n",
       "204       0.0323       0.0101       0.0298       0.0564       0.0760   \n",
       "205       0.0522       0.0437       0.0180       0.0292       0.0351   \n",
       "206       0.0303       0.0353       0.0490       0.0608       0.0167   \n",
       "207       0.0260       0.0363       0.0136       0.0272       0.0214   \n",
       "\n",
       "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
       "1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
       "2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
       "3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
       "4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
       "..           ...          ...          ...          ...           ...  ...   \n",
       "203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n",
       "204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n",
       "205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n",
       "206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n",
       "207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n",
       "\n",
       "     attribute_53  attribute_54  attribute_55  attribute_56  attribute_57  \\\n",
       "0          0.0027        0.0065        0.0159        0.0072        0.0167   \n",
       "1          0.0084        0.0089        0.0048        0.0094        0.0191   \n",
       "2          0.0232        0.0166        0.0095        0.0180        0.0244   \n",
       "3          0.0121        0.0036        0.0150        0.0085        0.0073   \n",
       "4          0.0031        0.0054        0.0105        0.0110        0.0015   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203        0.0116        0.0098        0.0199        0.0033        0.0101   \n",
       "204        0.0061        0.0093        0.0135        0.0063        0.0063   \n",
       "205        0.0160        0.0029        0.0051        0.0062        0.0089   \n",
       "206        0.0086        0.0046        0.0126        0.0036        0.0035   \n",
       "207        0.0146        0.0129        0.0047        0.0039        0.0061   \n",
       "\n",
       "     attribute_58  attribute_59  attribute_60  attribute_61  Class  \n",
       "0          0.0180        0.0084        0.0090        0.0032      R  \n",
       "1          0.0140        0.0049        0.0052        0.0044      R  \n",
       "2          0.0316        0.0164        0.0095        0.0078      R  \n",
       "3          0.0050        0.0044        0.0040        0.0117      R  \n",
       "4          0.0072        0.0048        0.0107        0.0094      R  \n",
       "..            ...           ...           ...           ...    ...  \n",
       "203        0.0065        0.0115        0.0193        0.0157      M  \n",
       "204        0.0034        0.0032        0.0062        0.0067      M  \n",
       "205        0.0140        0.0138        0.0077        0.0031      M  \n",
       "206        0.0034        0.0079        0.0036        0.0048      M  \n",
       "207        0.0040        0.0036        0.0061        0.0115      M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset\n",
    "df_sonar=pd.read_csv(\"sonar.csv\")\n",
    "df_sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_1     0\n",
      "attribute_2     0\n",
      "attribute_3     0\n",
      "attribute_4     0\n",
      "attribute_5     0\n",
      "               ..\n",
      "attribute_58    0\n",
      "attribute_59    0\n",
      "attribute_60    0\n",
      "attribute_61    0\n",
      "Class           0\n",
      "Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check null values\n",
    "print(df_sonar.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attribute_1     float64\n",
       "attribute_2     float64\n",
       "attribute_3     float64\n",
       "attribute_4     float64\n",
       "attribute_5     float64\n",
       "                 ...   \n",
       "attribute_58    float64\n",
       "attribute_59    float64\n",
       "attribute_60    float64\n",
       "attribute_61    float64\n",
       "Class            object\n",
       "Length: 61, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sonar.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df_sonar.isnull())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>attribute_61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "count   208.000000   208.000000   208.000000   208.000000   208.000000   \n",
       "mean      0.029164     0.038437     0.043832     0.053892     0.075202   \n",
       "std       0.022991     0.032960     0.038428     0.046528     0.055552   \n",
       "min       0.001500     0.000600     0.001500     0.005800     0.006700   \n",
       "25%       0.013350     0.016450     0.018950     0.024375     0.038050   \n",
       "50%       0.022800     0.030800     0.034300     0.044050     0.062500   \n",
       "75%       0.035550     0.047950     0.057950     0.064500     0.100275   \n",
       "max       0.137100     0.233900     0.305900     0.426400     0.401000   \n",
       "\n",
       "       attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "count   208.000000   208.000000   208.000000   208.000000    208.000000  ...   \n",
       "mean      0.104570     0.121747     0.134799     0.178003      0.208259  ...   \n",
       "std       0.059105     0.061788     0.085152     0.118387      0.134416  ...   \n",
       "min       0.010200     0.003300     0.005500     0.007500      0.011300  ...   \n",
       "25%       0.067025     0.080900     0.080425     0.097025      0.111275  ...   \n",
       "50%       0.092150     0.106950     0.112100     0.152250      0.182400  ...   \n",
       "75%       0.134125     0.154000     0.169600     0.233425      0.268700  ...   \n",
       "max       0.382300     0.372900     0.459000     0.682800      0.710600  ...   \n",
       "\n",
       "       attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n",
       "count    208.000000    208.000000    208.000000    208.000000    208.000000   \n",
       "mean       0.016069      0.013420      0.010709      0.010941      0.009290   \n",
       "std        0.012008      0.009634      0.007060      0.007301      0.007088   \n",
       "min        0.000000      0.000800      0.000500      0.001000      0.000600   \n",
       "25%        0.008425      0.007275      0.005075      0.005375      0.004150   \n",
       "50%        0.013900      0.011400      0.009550      0.009300      0.007500   \n",
       "75%        0.020825      0.016725      0.014900      0.014500      0.012100   \n",
       "max        0.100400      0.070900      0.039000      0.035200      0.044700   \n",
       "\n",
       "       attribute_57  attribute_58  attribute_59  attribute_60  attribute_61  \n",
       "count    208.000000    208.000000    208.000000    208.000000    208.000000  \n",
       "mean       0.008222      0.007820      0.007949      0.007941      0.006507  \n",
       "std        0.005736      0.005785      0.006470      0.006181      0.005031  \n",
       "min        0.000400      0.000300      0.000300      0.000100      0.000600  \n",
       "25%        0.004400      0.003700      0.003600      0.003675      0.003100  \n",
       "50%        0.006850      0.005950      0.005800      0.006400      0.005300  \n",
       "75%        0.010575      0.010425      0.010350      0.010325      0.008525  \n",
       "max        0.039400      0.035500      0.044000      0.036400      0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sonar.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the count of each class in target vraiables\n",
    "sns.countplot(x=\"Class\",data=df_sonar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the input output variables\n",
    "df_x=df_sonar.drop(columns=[\"Class\"])\n",
    "df_y=df_sonar[[\"Class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DataScience\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "l=LabelEncoder()\n",
    "l.fit(df_y)\n",
    "y=l.transform(df_y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x=np.array(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def maxf1_score(clf,df_x,y):\n",
    "    maxf=0\n",
    "    rs=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(df_x, y,random_state = r_state,test_size=0.20,stratify=y)\n",
    "        clf.fit(x_train,y_train)\n",
    "        y_pred=clf.predict(x_test)\n",
    "        tmp=f1_score(y_test,y_pred)\n",
    "        #print(\"random state :\",r_state,\" and f1 score: \",tmp)\n",
    "        if tmp>maxf:\n",
    "            maxf=tmp\n",
    "            rs=r_state\n",
    "    print(\"maximum f1_score is at random state :\",rs,\" and it is :\",maxf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models=pd.DataFrame(columns=[\"model\",\"f1_score\",\"std\"])\n",
    "models=list()\n",
    "f1_scr=list()\n",
    "std=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum f1_score is at random state : 82  and it is : 0.9\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "maxf1_score(lr,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39955135, -0.04064823, -0.02692565, ...,  0.06987027,\n",
       "         0.17167808, -0.65894689],\n",
       "       [ 0.70353822,  0.42163039,  1.05561832, ..., -0.47240644,\n",
       "        -0.44455424, -0.41985233],\n",
       "       [-0.12922901,  0.60106749,  1.72340448, ...,  1.30935987,\n",
       "         0.25276128,  0.25758223],\n",
       "       ...,\n",
       "       [ 1.00438083,  0.16007801, -0.67384349, ...,  0.90652575,\n",
       "        -0.03913824, -0.67887143],\n",
       "       [ 0.04953255, -0.09539176,  0.13480381, ..., -0.00759783,\n",
       "        -0.70402047, -0.34015415],\n",
       "       [-0.13794908, -0.06497869, -0.78861924, ..., -0.6738235 ,\n",
       "        -0.29860448,  0.99479044]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting on same scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "df_x=sc.fit_transform(df_x)\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f1 score for logsistic after stratified cross validation :  0.6141075327530948\n",
      "standard deviation for logistic from mean f1 score is :  0.1274697768255824\n",
      "array of f1 scores :  [0.48979592 0.62857143 0.71794872 0.7826087  0.4516129 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "f1_score_logistic=np.array([])\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(df_x, y):\n",
    "    x_train, x_test = df_x[train_index], df_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lr.fit(x_train,y_train)\n",
    "    y_pred=lr.predict(x_test)\n",
    "    tmp=f1_score(y_test,y_pred)\n",
    "    f1_score_logistic=np.append(f1_score_logistic,[tmp])\n",
    "print(\"mean f1 score for logsistic after stratified cross validation : \",np.mean(f1_score_logistic))\n",
    "print(\"standard deviation for logistic from mean f1 score is : \",np.std(f1_score_logistic))\n",
    "print(\"array of f1 scores : \",f1_score_logistic)\n",
    "models.append(\"logistic\")\n",
    "f1_scr.append(np.mean(f1_score_logistic))\n",
    "std.append(np.std(f1_score_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum f1_score is at random state : 58  and it is : 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "# decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc=DecisionTreeClassifier()\n",
    "maxf1_score(dc,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f1 score for decision after stratified cross validation :  0.6065088593576966\n",
      "standard deviation for decision from mean f1 score is :  0.07957370698477918\n",
      "array of f1 scores :  [0.55813953 0.5625     0.55       0.76190476 0.6       ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "f1_score_dc=np.array([])\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(df_x, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = df_x[train_index], df_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dc.fit(x_train,y_train)\n",
    "    y_pred=dc.predict(x_test)\n",
    "    tmp=f1_score(y_test,y_pred)\n",
    "    f1_score_dc=np.append(f1_score_dc,[tmp])\n",
    "print(\"mean f1 score for decision after stratified cross validation : \",np.mean(f1_score_dc))\n",
    "print(\"standard deviation for decision from mean f1 score is : \",np.std(f1_score_dc))\n",
    "print(\"array of f1 scores : \",f1_score_dc)\n",
    "models.append(\"decision treee\")\n",
    "f1_scr.append(np.mean(f1_score_dc))\n",
    "std.append(np.std(f1_score_dc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kc=KNeighborsClassifier()\n",
    "neighbors={\"n_neighbors\":range(1,30)}\n",
    "clf = GridSearchCV(kc, neighbors, cv=5,scoring=\"f1\")\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum f1_score is at random state : 61  and it is : 0.9500000000000001\n"
     ]
    }
   ],
   "source": [
    "#KNN classifier with n_neighbors=3\n",
    "kc=KNeighborsClassifier(n_neighbors=3)\n",
    "maxf1_score(kc,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f1 score for KNN after stratified cross validation :  0.5892054627165234\n",
      "standard deviation for KNN from mean f1 score is :  0.18539457796060815\n",
      "array of f1 scores :  [0.58536585 0.71794872 0.51282051 0.8372093  0.29268293]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "f1_score_kc=np.array([])\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(df_x, y):\n",
    "    x_train, x_test = df_x[train_index], df_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    kc.fit(x_train,y_train)\n",
    "    y_pred=kc.predict(x_test)\n",
    "    tmp=f1_score(y_test,y_pred)\n",
    "    f1_score_kc=np.append(f1_score_kc,[tmp])\n",
    "print(\"mean f1 score for KNN after stratified cross validation : \",np.mean(f1_score_kc))\n",
    "print(\"standard deviation for KNN from mean f1 score is : \",np.std(f1_score_kc))\n",
    "print(\"array of f1 scores : \",f1_score_kc)\n",
    "models.append(\"KNN\")\n",
    "f1_scr.append(np.mean(f1_score_kc))\n",
    "std.append(np.std(f1_score_kc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svc=SVC()\n",
    "parameters={\"kernel\":[\"linear\", \"poly\", \"rbf\"],\"C\":[0.001,0.01,0.1,1,10]}\n",
    "clf = GridSearchCV(svc, parameters, cv=5,scoring=\"f1\")\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum f1_score is at random state : 91  and it is : 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "svc=SVC(kernel=\"linear\",C=10)\n",
    "maxf1_score(svc,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f1 score for SVM after stratified cross validation :  0.6143426962425679\n",
      "standard deviation for SVM from mean f1 score is :  0.17324736746962494\n",
      "array of f1 scores :  [0.45454545 0.52631579 0.68292683 0.92307692 0.48484848]\n"
     ]
    }
   ],
   "source": [
    "#Computing mean f1 score and standard deviation using cross val_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "f1_score_svc=np.array([])\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(df_x, y):\n",
    "    x_train, x_test = df_x[train_index], df_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    svc.fit(x_train,y_train)\n",
    "    y_pred=svc.predict(x_test)\n",
    "    tmp=f1_score(y_test,y_pred)\n",
    "    f1_score_svc=np.append(f1_score_svc,[tmp])\n",
    "print(\"mean f1 score for SVM after stratified cross validation : \",np.mean(f1_score_svc))\n",
    "print(\"standard deviation for SVM from mean f1 score is : \",np.std(f1_score_svc))\n",
    "print(\"array of f1 scores : \",f1_score_svc)\n",
    "models.append(\"SVM\")\n",
    "f1_scr.append(np.mean(f1_score_svc))\n",
    "std.append(np.std(f1_score_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM model is performing the best so it is the final model\n",
    "x_train,x_test,y_train,y_test=train_test_split(df_x, y,random_state = 91,test_size=0.20,stratify=y)\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred=svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sonar_svm.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the model as pickle\n",
    "import joblib \n",
    "joblib.dump(svc, 'sonar_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
